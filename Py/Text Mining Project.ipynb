{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ref : https://github.com/luisfredgs/LSA-Text-Summarization/blob/master/base_summarizer.py\n",
    "# Base summarizer \n",
    "\n",
    "from operator import attrgetter\n",
    "from collections import namedtuple\n",
    "from utils import ItemsCount\n",
    "\n",
    "SentenceInfo = namedtuple(\"SentenceInfo\", (\"sentence\", \"order\", \"rating\",))\n",
    "\n",
    "class BaseSummarizer(object):\n",
    "    \n",
    "    def __call__(self, document, sentences_count):\n",
    "        raise NotImplementedError(\"This method should be overriden in subclass\")\n",
    "\n",
    "    @staticmethod\n",
    "    def normalize_word(word):\n",
    "        return word.lower()\n",
    "\n",
    "    @staticmethod\n",
    "    def _get_best_sentences(sentences, count, rating, *args, **kwargs):\n",
    "        rate = rating\n",
    "        if isinstance(rating, dict):\n",
    "            assert not args and not kwargs\n",
    "            rate = lambda s: rating[s]\n",
    "\n",
    "        infos = (SentenceInfo(s, o, rate(s, *args, **kwargs))\n",
    "            for o, s in enumerate(sentences))\n",
    "\n",
    "        # sort sentences by rating in descending order\n",
    "        infos = sorted(infos, key=attrgetter(\"rating\"), reverse=True)\n",
    "        # get `count` first best rated sentences\n",
    "        if not isinstance(count, ItemsCount):\n",
    "            count = ItemsCount(count)\n",
    "        infos = count(infos)\n",
    "        # sort sentences by their order in document\n",
    "        infos = sorted(infos, key=attrgetter(\"order\"))\n",
    "\n",
    "        return tuple(i.sentence for i in infos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LSA-Text-Summarization\n",
    "from __future__ import absolute_import\n",
    "from __future__ import division, print_function, unicode_literals\n",
    "import math\n",
    "import numpy\n",
    "import nltk\n",
    "\n",
    "from warnings import warn\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from numpy.linalg import svd as singular_value_decomposition\n",
    "from base_summarizer import BaseSummarizer\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "class LsaSummarizer(BaseSummarizer):\n",
    "    MIN_DIMENSIONS = 3\n",
    "    REDUCTION_RATIO = 1/1\n",
    "\n",
    "    _stop_words = list(stopwords.words('english'))\n",
    "\n",
    "    @property\n",
    "    def stop_words(self):\n",
    "        return self._stop_words\n",
    "\n",
    "    @stop_words.setter\n",
    "    def stop_words(self, words):\n",
    "        self._stop_words = words\n",
    "\n",
    "    def __call__(self, document, sentences_count):\n",
    "\n",
    "        dictionary = self._create_dictionary(document)\n",
    "        \n",
    "        if not dictionary:\n",
    "            return ()\n",
    "\n",
    "        sentences = sent_tokenize(document)\n",
    "\n",
    "        matrix = self._create_matrix(document, dictionary)\n",
    "        matrix = self._compute_term_frequency(matrix)\n",
    "        u, sigma, v = singular_value_decomposition(matrix, full_matrices=False)\n",
    "\n",
    "        ranks = iter(self._compute_ranks(sigma, v))\n",
    "        return self._get_best_sentences(sentences, sentences_count,\n",
    "            lambda s: next(ranks))\n",
    "\n",
    "    def _create_dictionary(self, document):\n",
    "        \"\"\"Creates mapping key = word, value = row index\"\"\"\n",
    "\n",
    "        words = word_tokenize(document)\n",
    "        words = tuple(words)\n",
    "\n",
    "        words = map(self.normalize_word, words)\n",
    "\n",
    "        unique_words = frozenset(w for w in words if w not in self._stop_words)\n",
    "\n",
    "        return dict((w, i) for i, w in enumerate(unique_words))\n",
    "\n",
    "    def _create_matrix(self, document, dictionary):\n",
    "        \"\"\"\n",
    "        Creates matrix of shape where cells\n",
    "        contains number of occurences of words (rows) in senteces (cols).\n",
    "        \"\"\"\n",
    "        sentences = sent_tokenize(document)\n",
    "        words_count = len(dictionary)\n",
    "        sentences_count = len(sentences)\n",
    "        if words_count < sentences_count:\n",
    "            message = (\n",
    "                \"Number of words (%d) is lower than number of sentences (%d). \"\n",
    "                \"LSA algorithm may not work properly.\"\n",
    "            )\n",
    "            warn(message % (words_count, sentences_count))\n",
    "\n",
    "        matrix = numpy.zeros((words_count, sentences_count))\n",
    "        for col, sentence in enumerate(sentences):\n",
    "            words = word_tokenize(sentence)\n",
    "            for word in words:\n",
    "                # only valid words is counted (not stop-words, ...)\n",
    "                if word in dictionary:\n",
    "                    row = dictionary[word]\n",
    "                    matrix[row, col] += 1\n",
    "\n",
    "        return matrix\n",
    "\n",
    "    def _compute_term_frequency(self, matrix, smooth=0.4):\n",
    "        \"\"\"\n",
    "        Computes TF metrics for each sentence (column) in the given matrix and  normalize \n",
    "        the tf weights of all terms occurring in a document by the maximum tf in that document \n",
    "        according to ntf_{t,d} = a + (1-a)\\frac{tf_{t,d}}{tf_{max}(d)^{'}}.\n",
    "        \n",
    "        The smoothing term $a$ damps the contribution of the second term - which may be viewed \n",
    "        as a scaling down of tf by the largest tf value in $d$\n",
    "        \"\"\"\n",
    "        assert 0.0 <= smooth < 1.0\n",
    "\n",
    "        max_word_frequencies = numpy.max(matrix, axis=0)\n",
    "        rows, cols = matrix.shape\n",
    "        for row in range(rows):\n",
    "            for col in range(cols):\n",
    "                max_word_frequency = max_word_frequencies[col]\n",
    "                if max_word_frequency != 0:\n",
    "                    frequency = matrix[row, col]/max_word_frequency\n",
    "                    matrix[row, col] = smooth + (1.0 - smooth)*frequency\n",
    "\n",
    "        return matrix\n",
    "\n",
    "    def _compute_ranks(self, sigma, v_matrix):\n",
    "        assert len(sigma) == v_matrix.shape[0]\n",
    "\n",
    "        dimensions = max(LsaSummarizer.MIN_DIMENSIONS,\n",
    "            int(len(sigma)*LsaSummarizer.REDUCTION_RATIO))\n",
    "        powered_sigma = tuple(s**2 if i < dimensions else 0.0\n",
    "            for i, s in enumerate(sigma))\n",
    "\n",
    "        ranks = []\n",
    "        \n",
    "        for column_vector in v_matrix.T:\n",
    "            rank = sum(s*v**2 for s, v in zip(powered_sigma, column_vector))\n",
    "            ranks.append(math.sqrt(rank))\n",
    "\n",
    "        return ranks\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#LSA-Text-Summarization\n",
    "\n",
    "from lsa_summarizer import LsaSummarizer\n",
    "import nltk\n",
    "nltk.download(\"punkt\", quiet=True)\n",
    "nltk.download(\"stopwords\", quiet=True)\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "source_file = \"original_text.txt\"\n",
    "\n",
    "with open(source_file, \"r\", encoding='utf-8') as file:\n",
    "    text = file.readlines()\n",
    "\n",
    "\n",
    "\n",
    "summarizer = LsaSummarizer()\n",
    "\n",
    "stopwords = stopwords.words('portuguese')\n",
    "summarizer.stop_words = stopwords\n",
    "summary =summarizer(text[0], 3)\n",
    "\n",
    "print(\"====== Original text =====\")\n",
    "print(text)\n",
    "print(\"====== End of original text =====\")\n",
    "\n",
    "\n",
    "\n",
    "print(\"\\n========= Summary =========\")\n",
    "\n",
    "print(\" \".join(summary))\n",
    "print(\"========= End of summary =========\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
